{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import pprint\n",
    "\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torchutils\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outfit dataset\n",
    "Outfits are saved in three splits:\n",
    "\n",
    "```json\n",
    "train: \"train_no_dup.json\",\n",
    "valid: \"valid_no_dup.json\",\n",
    "test: \"test_no_dup.json\",\n",
    "```\n",
    "\n",
    "Each outfit in the `json` file has the following keys:\n",
    "\n",
    "```json\n",
    "['name', 'views', 'items', 'image', 'likes', 'date', 'set_url', 'set_id', 'desc']\n",
    "```\n",
    "\n",
    "Each item in an outfit has the following keys:\n",
    "\n",
    "```json\n",
    "['index', 'name', 'prices', 'likes', 'image', 'categoryid']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_type(item):\n",
    "    return item[\"index\"] - 1\n",
    "\n",
    "\n",
    "def get_item_id(item):\n",
    "    return item[\"image\"].split(\"tid=\")[-1]\n",
    "\n",
    "\n",
    "def load_json(fn):\n",
    "    with open(fn, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDir = \"release\"\n",
    "outputDir = \"processed\"\n",
    "\n",
    "CHECK_IMAGES = False\n",
    "MAKE_LMDB = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outfits: 21889 = 17316 + 1497 + 3076\n"
     ]
    }
   ],
   "source": [
    "trainOutfits = load_json(f\"{inputDir}/label/train_no_dup.json\")\n",
    "validOutfits = load_json(f\"{inputDir}/label/valid_no_dup.json\")\n",
    "testOutfits = load_json(f\"{inputDir}/label/test_no_dup.json\")\n",
    "\n",
    "allOutfits = trainOutfits + validOutfits + testOutfits\n",
    "print(\n",
    "    \"Number of outfits: {} = {} + {} + {}\".format(\n",
    "        len(allOutfits), len(trainOutfits), len(validOutfits), len(testOutfits)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example outfit:\n",
      "{'date': 'One month',\n",
      " 'desc': 'A fashion look from January 2017 by beebeely-look featuring Fuji, '\n",
      "         'Citizens of Humanity, casual, casualoutfit, Packandgo, winterjacket '\n",
      "         'and gearbest',\n",
      " 'image': 'http://ak1.polyvoreimg.com/cgi/img-set/cid/214181831/id/El8a99fQ5hG4HrPFO4xqOQ/size/y.jpg',\n",
      " 'items': [...],\n",
      " 'likes': 394,\n",
      " 'name': 'Casual',\n",
      " 'set_id': '214181831',\n",
      " 'set_url': 'http://www.polyvore.com/casual/set?id=214181831',\n",
      " 'views': 8743}\n",
      "Example item:\n",
      "{'categoryid': 4495,\n",
      " 'image': 'http://img2.polyvoreimg.com/cgi/img-thing?.out=jpg&size=m&tid=194508109',\n",
      " 'index': 1,\n",
      " 'likes': 10,\n",
      " 'name': 'mock neck embroidery suede sweatshirt',\n",
      " 'price': 24.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example outfit:\")\n",
    "pprint.pprint(allOutfits[0], depth=1)\n",
    "print(\"Example item:\")\n",
    "pprint.pprint(allOutfits[0][\"items\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 111,589\n",
      "Reuse ratio: 1.277 = 142,480 / 111,589\n",
      "Number of categories: 380\n",
      "Average number of items in an outfit: 6.51\n"
     ]
    }
   ],
   "source": [
    "MAX_SIZE = 8\n",
    "\n",
    "itemSet = set()\n",
    "itemImages = dict()\n",
    "categorySet = set()\n",
    "outfitSet = set()\n",
    "# {set_id}_{index}: {item_id}\n",
    "name2Id = dict()\n",
    "\n",
    "# item index in item list\n",
    "itemReIndex = []\n",
    "itemList = [set() for _ in range(MAX_SIZE)]\n",
    "\n",
    "num_items = 0\n",
    "for outfit in allOutfits:\n",
    "    set_id = outfit[\"set_id\"]\n",
    "    outfitSet.add(set_id)\n",
    "    num_items += len(outfit[\"items\"])\n",
    "    for item in outfit[\"items\"]:\n",
    "        categorySet.add(item[\"categoryid\"])\n",
    "        index = item[\"index\"]\n",
    "        name = \"{}_{}\".format(set_id, index)\n",
    "        # the unique id of item\n",
    "        item_id = get_item_id(item)\n",
    "        item_type = get_item_type(item)\n",
    "        name2Id[name] = item_id\n",
    "        itemSet.add(item_id)\n",
    "        # use index as item category\n",
    "        itemList[item_type].add(item_id)\n",
    "        if item_id not in itemImages:\n",
    "            itemImages[item_id] = []\n",
    "        itemImages[item_id].append(os.path.join(f\"{inputDir}/images\", \"{}/{}.jpg\".format(set_id, index)))\n",
    "\n",
    "print(\"Number of unique items: {:,}\".format(len(itemSet)))\n",
    "print(\"Reuse ratio: {:.3f} = {:,} / {:,}\".format(num_items / len(itemSet), num_items, len(itemSet)))\n",
    "print(\"Number of categories: {}\".format(len(categorySet)))\n",
    "print(\"Average number of items in an outfit: {:.2f}\".format(num_items / len(allOutfits)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create item list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to item list\n",
    "itemReIndex = []\n",
    "itemList = [list(items) for items in itemList]\n",
    "for i in range(MAX_SIZE):\n",
    "    items = itemList[i]\n",
    "    item_index = {item_id: i for i, item_id in enumerate(items)}\n",
    "    itemReIndex.append(item_index)\n",
    "\n",
    "torchutils.io.save_json(f\"{outputDir}/original/items.json\", itemList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check items\n",
    "\n",
    "Since one image maybe used multiple times. Check whether the content is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all compatibility outfits are in allOutfits\n",
    "# name format: {set_id}_{item_index}\n",
    "\n",
    "with open(f\"{inputDir}/label/fashion_compatibility_prediction.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    set_id = line.split(\" \")[1].split(\"_\")[0]\n",
    "    assert set_id in outfitSet\n",
    "\n",
    "# all fitb outfits are in allOutfits\n",
    "# name format: {set_id}_{item_index}\n",
    "# {\"question\": [names, ...], \"answers\": [names, ], \"blank_position\": n}\n",
    "with open(f\"{inputDir}/label/fill_in_blank_test.json\") as f:\n",
    "    data = json.load(f)\n",
    "for d in data:\n",
    "    position = d[\"blank_position\"]\n",
    "    question = d[\"question\"]\n",
    "    for q in question:\n",
    "        set_id = q.split(\"_\")[0]\n",
    "        assert set_id in outfitSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_reuse_images():\n",
    "    itemImages = dict()\n",
    "    for k, v in tqdm(itemImages.items()):\n",
    "        if len(v) > 1:\n",
    "            images = []\n",
    "            for fn in v:\n",
    "                with open(fn, \"rb\") as f:\n",
    "                    images.append(np.array(PIL.Image.open(f).convert(\"RGB\")))\n",
    "            itemImages[k] = images\n",
    "\n",
    "    error = dict()\n",
    "    for k, v in tqdm(itemImages.items()):\n",
    "        imgs = np.stack(v)\n",
    "        mean = (imgs - imgs.mean(axis=0)).mean()\n",
    "        error[k] = mean\n",
    "    print(\"Mean error: {:.3f}\".format(np.array(list(error.values())).mean()))\n",
    "\n",
    "\n",
    "if CHECK_IMAGES:\n",
    "    check_reuse_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all images to LMDB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_LMDB:\n",
    "    dst = f\"{outputDir}/features/images\"\n",
    "    env = lmdb.open(dst, map_size=2 ** 40)\n",
    "    # open json file\n",
    "    with env.begin(write=True) as txn:\n",
    "        for item_id, item_path in tqdm(itemImages.items()):\n",
    "            fn = item_path[0]\n",
    "            with open(fn, \"rb\") as f:\n",
    "                img_data = f.read()\n",
    "                txn.put(item_id.encode(\"ascii\"), img_data)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Words informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 2756, plus 1 unkown\n"
     ]
    }
   ],
   "source": [
    "words = dict()\n",
    "with open(f\"{inputDir}/final_word_dict.txt\") as f:\n",
    "    for l in f.readlines():\n",
    "        k, v = l.strip().split()\n",
    "        words[k] = int(v)\n",
    "wordDict = dict()\n",
    "for w in words:\n",
    "    wordDict[w] = len(wordDict)\n",
    "print(\"Number of words: {}, plus 1 unkown\".format(len(wordDict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemWords = dict()\n",
    "for outfit in allOutfits:\n",
    "    for item in outfit[\"items\"]:\n",
    "        item_id = item[\"image\"].split(\"tid=\")[-1]\n",
    "        embd = [0] * (len(wordDict) + 1)\n",
    "        for w in item[\"name\"].split():\n",
    "            if w in wordDict:\n",
    "                embd[wordDict[w]] += 1\n",
    "            else:\n",
    "                embd[-1] += 1\n",
    "        itemWords[item_id] = np.array(embd)\n",
    "with open(f\"{outputDir}/word_embedding.pkl\", \"wb\") as f:\n",
    "    pkl.dump(itemWords, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to outfit to tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tuples(outfits):\n",
    "    tuples = []\n",
    "    for outfit in outfits:\n",
    "        items = [-1] * MAX_SIZE\n",
    "        types = [-1] * MAX_SIZE\n",
    "        size = len(outfit[\"items\"])\n",
    "        for i, item in enumerate(outfit[\"items\"]):\n",
    "            tid = item[\"image\"].split(\"tid=\")[-1]\n",
    "            item_type = item[\"index\"] - 1\n",
    "            item_index = itemReIndex[item_type][tid]\n",
    "            items[i] = item_index\n",
    "            types[i] = item_type\n",
    "        tuples.append([0] + [size] + items + types)\n",
    "    return np.array(tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTuples = convert_to_tuples(trainOutfits)\n",
    "validTuples = convert_to_tuples(validOutfits)\n",
    "testTuples = convert_to_tuples(testOutfits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{outputDir}/original/train_pos\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(trainTuples)\n",
    "\n",
    "with open(f\"{outputDir}/original/valid_pos\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(validTuples)\n",
    "\n",
    "with open(f\"{outputDir}/original/test_pos\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(testTuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert FITB and Negative tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_compatibility(data):\n",
    "    eval_pos = []\n",
    "    eval_neg = []\n",
    "\n",
    "    for line in data:\n",
    "        label, *names = line.split()\n",
    "        size = len(names)\n",
    "        m = MAX_SIZE - size\n",
    "        types = [int(name.split(\"_\")[1]) - 1 for name in names]\n",
    "        items = [itemReIndex[c][name2Id[i]] for i, c in zip(names, types)]\n",
    "        tpl = [0, size] + items + [-1] * m + types + [-1] * m\n",
    "        if int(label) == 1:\n",
    "            eval_pos.append(tpl)\n",
    "        else:\n",
    "            eval_neg.append(tpl)\n",
    "    eval_pos = np.array(eval_pos)\n",
    "    eval_neg = np.array(eval_neg)\n",
    "    return eval_pos, eval_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive outfits: 3,076\n",
      "Number of negative outfits: 4,000\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{inputDir}/label/fashion_compatibility_prediction.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "eval_pos, eval_neg = convert_compatibility(lines)\n",
    "# the positive tuples should match those in compatibility\n",
    "assert (testTuples == eval_pos).all()\n",
    "print(\"Number of positive outfits: {:,}\".format(len(eval_pos)))\n",
    "print(\"Number of negative outfits: {:,}\".format(len(eval_neg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{outputDir}/original/test_neg\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(eval_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fitb(data):\n",
    "    tuples = []\n",
    "    for d in data:\n",
    "        position = d[\"blank_position\"]\n",
    "        question = d[\"question\"]\n",
    "        question_types = [int(s.split(\"_\")[1]) - 1 for s in question]\n",
    "        question_items = [itemReIndex[c][name2Id[i]] for i, c in zip(question, question_types)]\n",
    "        size = len(question) + 1\n",
    "        m = MAX_SIZE - size\n",
    "        for ans in d[\"answers\"]:\n",
    "            c = int(ans.split(\"_\")[-1]) - 1\n",
    "            i = itemReIndex[c][name2Id[ans]]\n",
    "            items = question_items.copy()\n",
    "            types = question_types.copy()\n",
    "            items.insert(position - 1, i)\n",
    "            types.insert(position - 1, c)\n",
    "            tuples.append([0, size] + items + [-1] * m + types + [-1] * m)\n",
    "    tuples = np.array(tuples)\n",
    "    return tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{inputDir}/label/fill_in_blank_test.json\") as f:\n",
    "    data = json.load(f)\n",
    "    tuples = convert_fitb(data)\n",
    "\n",
    "with open(f\"{outputDir}/original/test_fitb\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tuples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuples from type-aware embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = f\"{outputDir}/hardneg\"\n",
    "src_dir = f\"{inputDir}/maryland_polyvore_hardneg/\"\n",
    "\n",
    "torchutils.io.save_json(f\"{dst_dir}/items.json\", itemList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive outfits (train): 17,316\n",
      "Number of negative outfits (train): 17,192\n",
      "Number of positive outfits (valid): 1,497\n",
      "Number of negative outfits (valid): 1,467\n",
      "Number of positive outfits (test): 3,076\n",
      "Number of negative outfits (test): 3,005\n"
     ]
    }
   ],
   "source": [
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "outfits = dict(train=trainTuples, valid=validTuples, test=testTuples)\n",
    "for phase in splits:\n",
    "    fn = os.path.join(src_dir, \"compatibility_{}.txt\".format(phase))\n",
    "    with open(fn) as f:\n",
    "        data = f.readlines()\n",
    "    pos_tuples, neg_tuples = convert_compatibility(data)\n",
    "    assert (pos_tuples == outfits[phase]).all()\n",
    "    torchutils.io.save_csv(os.path.join(dst_dir, \"{}_neg\".format(phase)), neg_tuples)\n",
    "    torchutils.io.save_csv(os.path.join(dst_dir, \"{}_pos\".format(phase)), pos_tuples)\n",
    "    print(\"Number of positive outfits ({}): {:,}\".format(phase, len(pos_tuples)))\n",
    "    print(\"Number of negative outfits ({}): {:,}\".format(phase, len(neg_tuples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions (train): 17,316\n",
      "Number of questions (valid): 1,497\n",
      "Number of questions (test): 3,076\n"
     ]
    }
   ],
   "source": [
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "for phase in splits:\n",
    "    data = torchutils.io.load_json(os.path.join(src_dir, \"fill_in_blank_{}.json\".format(phase)))\n",
    "    tuples = convert_fitb(data)\n",
    "    torchutils.io.save_csv(os.path.join(dst_dir, \"{}_fitb\".format(phase)), tuples)\n",
    "    print(\"Number of questions ({}): {:,}\".format(phase, len(tuples) // 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05658dfe50cff6bbd42299af9f1a1eb42a9cf1a7abd2f1ef93f6f5316928c930"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "8895b83a8abffe789c6d593b9f007496d361a613987b7affdcaade89838bb702"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
